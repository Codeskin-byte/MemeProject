{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import things needed for Tensorflow and CoreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __builtin__ import any as b_any\n",
    "\n",
    "import math\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import configuration\n",
    "import inference_wrapper\n",
    "import sys\n",
    "sys.path.insert(0, 'im2txt/inference_utils')\n",
    "sys.path.insert(0, 'im2txt/ops')\n",
    "import caption_generator\n",
    "import image_processing\n",
    "import vocabulary\n",
    "\n",
    "import urllib, os, sys, zipfile\n",
    "from os.path import dirname\n",
    "from tensorflow.core.framework import graph_pb2\n",
    "from tensorflow.python.tools.freeze_graph import freeze_graph\n",
    "from tensorflow.python.tools import strip_unused_lib\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.platform import gfile\n",
    "import tfcoreml\n",
    "import configuration\n",
    "from coremltools.proto import NeuralNetwork_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "# Turn on debugging on error\n",
    "%pdb off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the models\n",
    "\n",
    "Create the Tensorflow model and strip all unused nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file = './trainlogIncNEW/model.ckpt-1000000'\n",
    "pre_frozen_model_file = './frozen_model_textgenCUSTOM.pb'\n",
    "frozen_model_file = './frozen_model_textgenCUSTOM.pb'\n",
    "\n",
    "# Which nodes we want to input for the network\n",
    "# Use ['image_feed'] for just Memeception\n",
    "input_node_names = ['seq_embeddings','lstm/state_feed']\n",
    "\n",
    "# Which nodes we want to output from the network\n",
    "# Use ['lstm/initial_state'] for just Memeception\n",
    "output_node_names = ['softmax','lstm/state']\n",
    "\n",
    "# Set the depth of the beam search\n",
    "beam_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Building model.\n",
      "About to decide if splitting\n",
      "FIRST (1, 2048)\n",
      "{'num_or_size_splits': 4, 'value': <tf.Tensor 'lstm/basic_lstm_cell/BiasAdd:0' shape=(1, 2048) dtype=float32>, 'axis': 1}\n",
      "new_h Tensor(\"lstm/basic_lstm_cell/Mul_2:0\", shape=(1, 512), dtype=float32)\n",
      "new_state LSTMStateTuple(c=<tf.Tensor 'lstm/basic_lstm_cell/Add_1:0' shape=(1, 512) dtype=float32>, h=<tf.Tensor 'lstm/basic_lstm_cell/Mul_2:0' shape=(1, 512) dtype=float32>)\n",
      "About to decide if splitting\n",
      "SECOND (2, 2048)\n",
      "{'num_or_size_splits': 4, 'value': <tf.Tensor 'lstm/basic_lstm_cell/BiasAdd_1:0' shape=(2, 2048) dtype=float32>, 'axis': 1}\n",
      "new_h Tensor(\"lstm/basic_lstm_cell/Mul_5:0\", shape=(1, 2, 512), dtype=float32)\n",
      "new_state LSTMStateTuple(c=<tf.Tensor 'lstm/basic_lstm_cell/Add_3:0' shape=(1, 2, 512) dtype=float32>, h=<tf.Tensor 'lstm/basic_lstm_cell/Mul_5:0' shape=(1, 2, 512) dtype=float32>)\n",
      "lstm_outputs Tensor(\"lstm/basic_lstm_cell/Mul_5:0\", shape=(1, 2, 512), dtype=float32)\n",
      "state_output LSTMStateTuple(c=<tf.Tensor 'lstm/basic_lstm_cell/Add_3:0' shape=(1, 2, 512) dtype=float32>, h=<tf.Tensor 'lstm/basic_lstm_cell/Mul_5:0' shape=(1, 2, 512) dtype=float32>)\n",
      "BUILDING DENSE\n",
      "MATMUL(TENSORDOT) w/out SPLITTING\n"
     ]
    }
   ],
   "source": [
    "# Build the inference graph.\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    model = inference_wrapper.InferenceWrapper()\n",
    "    restore_fn = model.build_graph_from_config(configuration.ModelConfig(),\n",
    "                                               checkpoint_file)\n",
    "g.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47082275"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the graph\n",
    "\n",
    "tf_model_path = './log/pre_graph_textgenCUSTOM.pb'\n",
    "tf.train.write_graph(\n",
    "    g,\n",
    "    './log',\n",
    "    'pre_graph_textgenCUSTOM.pb',\n",
    "    as_text=False,\n",
    ")\n",
    "\n",
    "with open(tf_model_path, 'rb') as f:\n",
    "    serialized = f.read()\n",
    "tf.reset_default_graph()\n",
    "original_gdef = tf.GraphDef()\n",
    "original_gdef.ParseFromString(serialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip unused graph elements and serialize the output to file\n",
    "\n",
    "gdef = strip_unused_lib.strip_unused(\n",
    "        input_graph_def = original_gdef,\n",
    "        input_node_names = input_node_names,\n",
    "        output_node_names = output_node_names,\n",
    "        placeholder_type_enum = dtypes.float32.as_datatype_enum)\n",
    "# Save it to an output file\n",
    "with gfile.GFile(pre_frozen_model_file, 'wb') as f:\n",
    "    f.write(gdef.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./trainlogIncNEW/model.ckpt-1000000\n",
      "INFO:tensorflow:Froze 4 variables.\n",
      "Converted 4 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "# Freeze the graph with checkpoint data inside\n",
    "\n",
    "freeze_graph(input_graph=pre_frozen_model_file,\n",
    "             input_saver='',\n",
    "             input_binary=True,\n",
    "             input_checkpoint=checkpoint_file,\n",
    "             output_node_names=','.join(output_node_names),\n",
    "             restore_op_name='save/restore_all',\n",
    "             filename_tensor_name='save/Const:0',\n",
    "             output_graph=frozen_model_file,\n",
    "             clear_devices=True,\n",
    "             initializer_nodes='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the model\n",
    "\n",
    "Check that it is producing legit captions for *One does not simply*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing vocabulary from file: vocab4.txt\n",
      "INFO:tensorflow:Created vocabulary with 38521 words\n"
     ]
    }
   ],
   "source": [
    "# Configure the model and load the vocab\n",
    "\n",
    "config = configuration.ModelConfig()\n",
    "\n",
    "vocab_file ='vocab4.txt'\n",
    "vocab = vocabulary.Vocabulary(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model from checkpoint: ./trainlogIncNEW/model.ckpt-1000000\n",
      "INFO:tensorflow:Restoring parameters from ./trainlogIncNEW/model.ckpt-1000000\n",
      "INFO:tensorflow:Successfully loaded checkpoint: model.ckpt-1000000\n",
      "god is the biggest badass\n",
      "god is the biggest badass of the world\n",
      "creates all the male character in the workplace\n",
      "creates all the male character who wants to be able to be able to know what to do\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-777767a60810>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ALP/PycharmProjects/MemeProject/im2txt/inference_utils/caption_generator.pyc\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, sess, encoded_image)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \"\"\"\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# Feed in the image to get the initial state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     initial_beam = Caption(\n",
      "\u001b[0;32m/Users/ALP/PycharmProjects/MemeProject/im2txt/inference_wrapper.pyc\u001b[0m in \u001b[0;36mfeed_image\u001b[0;34m(self, sess, encoded_image)\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfeed_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     initial_state = sess.run(fetches=\"lstm/initial_state:0\",\n\u001b[0;32m---> 50\u001b[0;31m                              feed_dict={\"image_feed:0\": encoded_image})\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ALP/PycharmProjects/MemeProject/venv3/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ALP/PycharmProjects/MemeProject/venv3/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m/Users/ALP/PycharmProjects/MemeProject/venv3/lib/python2.7/site-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate captions on a hard-coded image\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "  restore_fn(sess)\n",
    "  generator = caption_generator.CaptionGenerator(\n",
    "      model, vocab, beam_size=beam_size)\n",
    "  for i,filename in enumerate(['memes/advice-god.jpg']):\n",
    "    with tf.gfile.GFile(filename, \"rb\") as f:\n",
    "      image = Image.open(f)\n",
    "      image = ((np.array(image.resize((299,299)))/255.0)-0.5)*2.0\n",
    "    for k in range(50):\n",
    "      captions = generator.beam_search(sess, image)    \n",
    "      for i, caption in enumerate(captions):\n",
    "        sentence = [vocab.id_to_word(w) for w in caption.sentence[1:-1]]\n",
    "        sentence = \" \".join(sentence)\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the model to CoreML\n",
    "\n",
    "Specify output variables from the graph to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basic shapes\n",
    "# If using Memeception, add 'image_feed:0': [299, 299, 3]\n",
    "input_tensor_shapes = {\n",
    "    'seq_embeddings:0': [1, beam_size, 300],\n",
    "    'lstm/state_feed:0': [1, beam_size, 1024],\n",
    "}\n",
    "\n",
    "coreml_model_file = './Textgen_CUSTOM.mlmodel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor_names = [node + ':0' for node in output_node_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightLSTM = np.loadtxt('weightLSTM')\n",
    "weightFully = np.loadtxt('weightFully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes not found for 40 tensors. Executing graph to determine shapes. \n",
      "Automatic shape interpretation succeeded for input blob lstm/state_feed:0\n",
      "1/52: Analysing op name: seq_embeddings ( type:  Placeholder )\n",
      "Skipping name of placeholder\n",
      "2/52: Analysing op name: lstm/basic_lstm_cell/kernel ( type:  Const )\n",
      "3/52: Analysing op name: lstm/basic_lstm_cell/kernel/read ( type:  Identity )\n",
      "4/52: Analysing op name: lstm/basic_lstm_cell/bias ( type:  Const )\n",
      "5/52: Analysing op name: lstm/basic_lstm_cell/bias/read ( type:  Identity )\n",
      "6/52: Analysing op name: lstm/state_feed ( type:  Placeholder )\n",
      "Skipping name of placeholder\n",
      "7/52: Analysing op name: lstm/ExpandDims/dim ( type:  Const )\n",
      "8/52: Analysing op name: lstm/ExpandDims ( type:  ExpandDims )\n",
      "9/52: Analysing op name: lstm/split/split_dim ( type:  Const )\n",
      "10/52: Analysing op name: lstm/split ( type:  Split )\n",
      "11/52: Analysing op name: lstm/Squeeze ( type:  Squeeze )\n",
      "12/52: Analysing op name: lstm/Squeeze_1 ( type:  Squeeze )\n",
      "13/52: Analysing op name: lstm/basic_lstm_cell/concat_1/axis ( type:  Const )\n",
      "14/52: Analysing op name: lstm/basic_lstm_cell/concat_1 ( type:  ConcatV2 )\n",
      "15/52: Analysing op name: lstm/basic_lstm_cell/Squeeze_8 ( type:  Squeeze )\n",
      "16/52: Analysing op name: lstm/basic_lstm_cell/LSTMmatmul2 ( type:  MatMul )\n",
      "Adding custom layer\n",
      "LSTM\n",
      "17/52: Analysing op name: lstm/basic_lstm_cell/BiasAdd_1 ( type:  BiasAdd )\n",
      "18/52: Analysing op name: lstm/basic_lstm_cell/ExpandDims_2/dim ( type:  Const )\n",
      "19/52: Analysing op name: lstm/basic_lstm_cell/ExpandDims_2 ( type:  ExpandDims )\n",
      "20/52: Analysing op name: lstm/basic_lstm_cell/ExpandDims_3/dim ( type:  Const )\n",
      "21/52: Analysing op name: lstm/basic_lstm_cell/ExpandDims_3 ( type:  ExpandDims )\n",
      "22/52: Analysing op name: lstm/basic_lstm_cell/split_1/split_dim ( type:  Const )\n",
      "23/52: Analysing op name: lstm/basic_lstm_cell/split_1 ( type:  Split )\n",
      "24/52: Analysing op name: lstm/basic_lstm_cell/Squeeze_9 ( type:  Squeeze )\n",
      "25/52: Analysing op name: lstm/basic_lstm_cell/Squeeze_10 ( type:  Squeeze )\n",
      "26/52: Analysing op name: lstm/basic_lstm_cell/Sigmoid_4 ( type:  Sigmoid )\n",
      "27/52: Analysing op name: lstm/basic_lstm_cell/Squeeze_11 ( type:  Squeeze )\n",
      "28/52: Analysing op name: lstm/basic_lstm_cell/Squeeze_12 ( type:  Squeeze )\n",
      "29/52: Analysing op name: lstm/basic_lstm_cell/Tanh_2 ( type:  Tanh )\n",
      "30/52: Analysing op name: lstm/basic_lstm_cell/Mul_4 ( type:  Mul )\n",
      "31/52: Analysing op name: lstm/basic_lstm_cell/Squeeze_13 ( type:  Squeeze )\n",
      "32/52: Analysing op name: lstm/basic_lstm_cell/Squeeze_14 ( type:  Squeeze )\n",
      "33/52: Analysing op name: lstm/basic_lstm_cell/Squeeze_15 ( type:  Squeeze )\n",
      "34/52: Analysing op name: lstm/basic_lstm_cell/Squeeze_16 ( type:  Squeeze )\n",
      "35/52: Analysing op name: lstm/basic_lstm_cell/Sigmoid_5 ( type:  Sigmoid )\n",
      "36/52: Analysing op name: lstm/basic_lstm_cell/Const_3 ( type:  Const )\n",
      "37/52: Analysing op name: lstm/basic_lstm_cell/Add_2 ( type:  Add )\n",
      "38/52: Analysing op name: lstm/basic_lstm_cell/Sigmoid_3 ( type:  Sigmoid )\n",
      "39/52: Analysing op name: lstm/basic_lstm_cell/Mul_3 ( type:  Mul )\n",
      "40/52: Analysing op name: lstm/basic_lstm_cell/Add_3 ( type:  Add )\n",
      "41/52: Analysing op name: lstm/basic_lstm_cell/Tanh_3 ( type:  Tanh )\n",
      "42/52: Analysing op name: lstm/basic_lstm_cell/Mul_5 ( type:  Mul )\n",
      "43/52: Analysing op name: logits/Squeeze ( type:  Squeeze )\n",
      "44/52: Analysing op name: lstm/state/axis ( type:  Const )\n",
      "45/52: Analysing op name: lstm/state ( type:  ConcatV2 )\n",
      "46/52: Analysing op name: logits/weights ( type:  Const )\n",
      "47/52: Analysing op name: logits/weights/read ( type:  Identity )\n",
      "48/52: Analysing op name: logits/Fullymatmul ( type:  MatMul )\n",
      "Adding custom layer\n",
      "Fully\n",
      "49/52: Analysing op name: logits/biases ( type:  Const )\n",
      "50/52: Analysing op name: logits/biases/read ( type:  Identity )\n",
      "51/52: Analysing op name: logits/BiasAdd ( type:  BiasAdd )\n",
      "52/52: Analysing op name: softmax ( type:  Softmax )\n",
      "\n",
      " Core ML model generated. Saved at location: ./Textgen_CUSTOM.mlmodel \n",
      "\n",
      "Core ML input(s): \n",
      " [name: \"seq_embeddings__0\"\n",
      "type {\n",
      "  multiArrayType {\n",
      "    shape: 300\n",
      "    shape: 1\n",
      "    shape: 2\n",
      "    dataType: DOUBLE\n",
      "  }\n",
      "}\n",
      ", name: \"lstm__state_feed__0\"\n",
      "type {\n",
      "  multiArrayType {\n",
      "    shape: 1024\n",
      "    shape: 1\n",
      "    shape: 2\n",
      "    dataType: DOUBLE\n",
      "  }\n",
      "}\n",
      "]\n",
      "Core ML output(s): \n",
      " [name: \"lstm__state__0\"\n",
      "type {\n",
      "  multiArrayType {\n",
      "    dataType: DOUBLE\n",
      "  }\n",
      "}\n",
      ", name: \"softmax__0\"\n",
      "type {\n",
      "  multiArrayType {\n",
      "    dataType: DOUBLE\n",
      "  }\n",
      "}\n",
      "]\n",
      "\n",
      "\n",
      "Custom layers have been added to the CoreML model corresponding to the following ops in the TF graph: \n",
      "1/2: op type: MatMul, op input names and shapes: [('lstm/basic_lstm_cell/Squeeze_8:0', [2, 812]), ('lstm/basic_lstm_cell/kernel/read:0', [812, 2048])], op output names and shapes: [('lstm/basic_lstm_cell/LSTMmatmul2:0', [2, 2048])]\n",
      "2/2: op type: MatMul, op input names and shapes: [('logits/Squeeze:0', [2, 512]), ('logits/weights/read:0', [512, 38521])], op output names and shapes: [('logits/Fullymatmul:0', [2, 38521])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def convert_matmul(**kwargs):\n",
    "    # Only convert this Lambda layer if it is for our swish function.\n",
    "    tf_op = kwargs[\"op\"]\n",
    "    if tf_op.name == 'lstm/basic_lstm_cell/LSTMmatmul2':\n",
    "        W = weightLSTM\n",
    "        print('LSTM')\n",
    "    else:\n",
    "        W = weightFully\n",
    "        print('Fully')\n",
    "    coreml_nn_builder = kwargs[\"nn_builder\"]\n",
    "    constant_inputs = kwargs[\"constant_inputs\"]\n",
    "    \n",
    "    params = NeuralNetwork_pb2.CustomLayerParams()\n",
    "\n",
    "    # The name of the Swift or Obj-C class that implements this layer.\n",
    "    params.className = \"MatMul\"\n",
    "\n",
    "    # The desciption is shown in Xcode's mlmodel viewer.\n",
    "    params.description = \"A fancy new matmul\"\n",
    "    \n",
    "    #W = constant_inputs.get(tf_op.inputs[1].name,[0,100,0,0])\n",
    "    #print(tf_op.inputs[1])\n",
    "    #size = constant_inputs.get(tf_op.inputs[2].name, [0,0,0,0])\n",
    "    # add begin and size as two repeated weight fields\n",
    "    for i,weightvec in enumerate(W):\n",
    "        W_as_weights = params.weights.add()\n",
    "        W_as_weights.floatValue.extend(map(float, weightvec))\n",
    "    #print(W_as_weights)\n",
    "    #size_as_weights = params.weights.add()\n",
    "    #size_as_weights.floatValue.extend(map(float, size))\n",
    "    coreml_nn_builder.add_custom(name=tf_op.name,\n",
    "                                input_names=[tf_op.inputs[0].name],\n",
    "                                output_names=[tf_op.outputs[0].name],\n",
    "                                custom_proto_spec=params)\n",
    "\n",
    "    #return params\n",
    "\n",
    "\n",
    "coreml_model = tfcoreml.convert(\n",
    "        tf_model_path=frozen_model_file, \n",
    "        mlmodel_path=coreml_model_file, \n",
    "        input_name_shape_dict=input_tensor_shapes,\n",
    "        output_feature_names=output_tensor_names,\n",
    "        add_custom_layers=True,\n",
    "        custom_conversion_functions={ \"lstm/basic_lstm_cell/LSTMmatmul2\": convert_matmul, \"logits/Fullymatmul\": convert_matmul}\n",
    "        #custom_conversion_functions={ \"MatMuldlskfjslkfj\": convert_matmul}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "Run a predictable randomly seeded inputs through and see where the disparities are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_rand = np.random.rand(300)\n",
    "seq_embeddings_tf = np.array([[seq_rand, seq_rand]])\n",
    "seq_embeddings_ml = np.array([[[sr, sr]] for sr in seq_rand])\n",
    "\n",
    "state_rand = np.random.rand(1024)\n",
    "state_feed_tf = np.array([[state_rand, state_rand]])\n",
    "state_feed_ml = np.array([[[sr, sr]] for sr in state_rand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_inputs = {\n",
    "    'seq_embeddings__0': seq_embeddings_ml,\n",
    "    'lstm__state_feed__0': state_feed_ml,\n",
    "}\n",
    "coreml_output = coreml_model.predict(coreml_inputs, useCPUOnly=True)\n",
    "# print(coreml_output['lstm__state__0'].shape)\n",
    "# print(coreml_output['softmax__0'].shape)\n",
    "# print(coreml_output['softmax__0'].reshape(38521, 1, 2))\n",
    "# print(coreml_output)\n",
    "def print_ml(ml):\n",
    "    for key in sorted(ml.keys()):\n",
    "        print(key)\n",
    "        print(ml[key].shape)\n",
    "        print(ml[key])\n",
    "print_ml(coreml_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    # Load the model from checkpoint.\n",
    "    restore_fn(sess)\n",
    "    input_names = ['lstm/state:0', 'softmax:0']\n",
    "    output_values = sess.run(\n",
    "        fetches=input_names,\n",
    "        feed_dict={\n",
    "            #\"input_feed:0\": input_feed,\n",
    "            \"lstm/state_feed:0\": state_feed_tf,\n",
    "            \"seq_embeddings:0\": seq_embeddings_tf,\n",
    "            #\"seq_embedding/embedding_map:0\": self.embedding_map\n",
    "        })\n",
    "    for (index, value) in sorted(enumerate(input_names), key=lambda x: x[1]):\n",
    "        print(value)\n",
    "        print(output_values[index].shape)\n",
    "        print(output_values[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(np.random.rand(1, 20), np.random.rand(20, 45)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(1, 2, 812)[0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
